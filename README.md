# Autonomous Robots

Lecture Notes and Experiments on Autonomous Robots

| **Topic** | **Note** | **Code** |
| :--- | :---: | :--- |
| ROS2 | [PDF](https://drive.google.com/file/d/1LaWyp9g2lKIGXSB5wsFIJdPkhkGgJtfK/view?usp=sharing) | [ROS2](ros2) |
| Navigation | [PDF](https://drive.google.com/file/d/1UzeNSkWWHATLXmAEcyfabvVfhwmRMu5V/view?usp=sharing) | - |
| Localization | [PDF](https://drive.google.com/file/d/1nyhgjG2B07__5PhBgBXe9yiMRfqCk8Bm/view?usp=sharing) | - |
| Perception | [PDF](https://drive.google.com/file/d/1tZcBpqq-5jJ2vID9ryoN6sflCSZ9sJsk/view?usp=sharing) | - |
| Visual Geometry | [PDF](https://drive.google.com/file/d/1mW31Csyx3tbVoknNvRaBG17z1x5YXhbv/view?usp=sharing) | - | 
| Kinematics | [PDF](https://drive.google.com/file/d/1BGR3RCgn05K2cvbFuy3s8DrGYGJIbtZS/view?usp=sharing) | - |


### Affordance

- [Learning Affordances at Inference-Time for Vision-Language-Action Models](https://arxiv.org/pdf/2510.19752)

### Agents in Robotics

- [RoboGen](https://github.com/Genesis-Embodied-AI/RoboGen)
- [OpenMind](https://openmind.org/)
- [Ark Framework](https://github.com/Robotics-Ark/ark_framework)



### Blogs
- [3D Perception by NVIDIA](https://developer.nvidia.com/blog/r2d2-building-ai-based-3d-robot-perception-and-mapping-with-nvidia-research/)

### Data Visualization

- [`rerun.io`](https://rerun.io/)
  
### Datasets

- [EgoDex](https://arxiv.org/pdf/2505.11709)
- [Princeton Digital Assets](https://princeton-vl.github.io/infinigen-sim/)
- [GraspGen](https://graspgen.github.io/)
- [ShareGPT-4o-Image](https://github.com/FreedomIntelligence/ShareGPT-4o-Image)
- [Wan Video Generation](https://github.com/Wan-Video/Wan2.2?tab=readme-ov-file)
- [RoboGen](https://arxiv.org/pdf/2311.01455)
- [Large Scale Grasp Dataset](https://research.nvidia.com/publication/2021-05_acronym-large-scale-grasp-dataset-based-simulation)
- [Rosario Agricultural Dataset](https://cifasis.github.io/rosariov2/)
- [Project Go-Big](https://www.figure.ai/news/project-go-big)
- [Behavior Challenge by Stanford](https://behavior.stanford.edu/index.html)
- [BridgeData V2: A Dataset for Robot Learning at Scale](https://rail-berkeley.github.io/bridgedata/)
- [Robocasa](https://robocasa.ai/) :star2:
- [Egocentric Dataset](https://build.ai/opensource)
- [ACRONYM: A Large-Scale Grasp Dataset Based on Simulation](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560844&casa_token=vgC8MR5HFdMAAAAA:75QBFeoIA_gzxBaQboj7SI5pIVsGL1uCRCeQOUo7QUypqqP8LUF8-IK5EIzTMqCuFqIx0BVBISgC&tag=1)
- [Spatial AI](https://huggingface.co/datasets/spatial-ai/sea-small)
- [In-N-On Scaling Egocentric manipulation with in-the-wild and on-task data](https://xiongyicai.github.io/In-N-On/)

### Depth Models

- [Depth Anything](https://arxiv.org/pdf/2511.10647)

### Development Kit

- [SunFounder](https://www.sunfounder.com/collections/robotics/products/picar-x)

### Diffusion in Robotics

- [Diffusion Policy: Visuomotor Policy Learning via Action Diffusion](https://arxiv.org/pdf/2303.04137)
- [Diffusion vs Autoregressive in Robotics](https://blog.ml.cmu.edu/2025/09/22/diffusion-beats-autoregressive-in-data-constrained-settings/)

### Development Kits for Robotics

- [Segway Nova Carter](https://robotics.segway.com/%20nova-carter/)


### Foundation Models

- [Perpeption Foundation Model](https://ai.meta.com/blog/meta-fair-updates-perception-localization-reasoning/)
- [Vision Language Model Book](https://books.google.com.ph/books?hl=en&lr=&id=EnqCEQAAQBAJ&oi=fnd&pg=PA228&ots=gd6NsKrxmc&sig=xrZ7-mB00G-w_QyRlAXGEhfLNAU&redir_esc=y#v=onepage&q&f=false)
- [Streaming VLM - Han Lab MIT](https://github.com/mit-han-lab/streaming-vlm)
  
### Grasping

- [FoundationGrasp and GraspGPT](https://github.com/mkt1412/GraspGPT_public)
- [Visual Dexterity: In-Hand Reorientation of Novel and Complex Object Shapes](https://taochenshh.github.io/projects/visual-dexterity)
- [AffordGrasp](https://arxiv.org/pdf/2503.00778)
- [Grasp Diffusion](https://github.com/robotgradient/grasp_diffusion)
- [Open-Vocabulary Affordance Detection in 3D Point Clouds](https://arxiv.org/pdf/2303.02401)
- [GraspGen](https://graspgen.github.io/)


### Humanoid 

- [OpenArm - Open Source Humanoid](https://github.com/enactic/OpenArm)
- [All Humanoids as of Oct 2025](https://spectrum.ieee.org/video-friday-robotic-hands-2674168909)
- [Awesome Humanoid Learning](https://github.com/jonyzhang2023/awesome-humanoid-learning)


#### Humanoid Dev Kits

- [Noetix Robotics](https://en.noetixrobotics.com/)
- [Humanoid Robots - Not Solution](https://www.forbes.com/sites/trondarneundheim/2025/12/01/humanoid-robots-wont-save-manufacturing-heres-what-will/)

### Inference Pipeline

- [DeepStream8](https://developer.nvidia.com/blog/build-a-real-time-visual-inspection-pipeline-with-nvidia-tao-6-and-nvidia-deepstream-8/)


### Joint Embedding

- [Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations](https://arxiv.org/pdf/2510.23607)


### Memory for Robotics

- [Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering](https://arxiv.org/pdf/2507.12846)
- [NVIDIA's ReMEmbR](https://nvidia-ai-iot.github.io/remembr)
- [FindingDory](https://arxiv.org/pdf/2506.15635)

### Motion Retargeting

- [General Motion Retargeting](https://github.com/YanjieZe/GMR)

### Neural Robots

- [Neural Robot Dynamics](https://arxiv.org/pdf/2508.15755)

### Papers

- [Hierarchical Reasoning Model](https://github.com/sapientinc/HRM/)
- [Imitating Humans](https://kimhanjung.github.io/UniSkill/)


### Pointing

- [Molmo2 AllenAI](https://playground.allenai.org/?model=molmo2-8b)

### Reinforcement Learning Framework

- [EasyR1](https://github.com/hiyouga/EasyR1)
- [R-Zero](https://github.com/Chengsong-Huang/R-Zero)
- [verl: Volcano Engine Reinforcement Learning for LLMs](https://github.com/volcengine/verl)

### Robotics Foundation Models

- [Gemini Robptics](https://arxiv.org/pdf/2510.03342)

### Simulation

- [Habitat](https://github.com/facebookresearch/habitat-lab/)
- [Newton](https://developer.nvidia.com/newton-physics)
- [Viral Humanoid](https://viral-humanoid.github.io/)

### Skin

- [Intrinsic sense of touch for intuitive physical human-robot interaction](https://www.science.org/doi/10.1126/scirobotics.adn4008)

### SLAM

- [Handbook](http://asrl.utias.utoronto.ca/~tdb/slam/)

### Tracking

- [3D Object Tracking](https://light.princeton.edu/publication/inverse-rendering-tracking/)
- [Person Tracking (Sports)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb)
  


### Vision

- [Dinov3](https://ai.meta.com/blog/dinov3-self-supervised-vision-model)
- [Map Anything](https://map-anything.github.io/)

### Vision Language Action (VLA)

- [OpenPi](https://github.com/Physical-Intelligence/openpi)
- [State of VLA 2026](https://mbreuss.github.io/blog_post_iclr_26_vla.html)

### World Models

- [NVIDIA Cosmos Reasoning](https://research.nvidia.com/labs/dir/cosmos-reason1/)
- [DreamgGem by NVIDIA](https://research.nvidia.com/labs/gear/dreamgen/)
- [Genesis RoboGen](https://github.com/Genesis-Embodied-AI/RoboGen)
- [Wan2.2](https://github.com/Wan-Video/Wan2.2)
- [Awesome World Models](https://github.com/knightnemo/Awesome-World-Models)
- [NVIDIA Cosmos Cookbook](https://nvidia-cosmos.github.io/cosmos-cookbook/index.html)
- [Robot Learning from a Physical World Model](https://arxiv.org/pdf/2511.07416)
